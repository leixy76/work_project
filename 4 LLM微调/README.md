## 模型训练

在技术性标书生成过程中，主要面临两大挑战：首先，标题通常简短且信息有限，导致从相关文件中难以检索到足够的信息；其次，如何基于检索到的内容生成详细且连贯的长段落输出。

1. 标题简短、信息有限的问题
*指令扩展*：根据标题进行问题拓展，大模型可以预测用户可能期待的指令，该指令通过限制写作方向来使内容更加具体，从而对标题进行有效扩展。
*模块化检索*：不依赖语义匹配，仅通过大模型判断当前段落与指令的相关性,请参考个人项目。
2. 增强长段落输出的问题
*微调模型*：为了生成详细且连贯的长段落输出，微调模型具有长文本输出的能力。

#### **数据合成**

+ `formulate`
  **如何定义标书/论文生成流程**
  从人类的角度来看，制定标书生成（`formulate`）的过程可以分为以下几个关键步骤：
    ```mermaid
    graph LR
        A[需求说明] --> B[检索信息]
        B --> C[制定大纲]
        C --> D[选择标题]
        D --> E[附件检索]
        E --> F[生成段落]
    ```
  由于标题长度有限，信息不足，导致难以有效检索附件信息。为此，我们引入伪指令生成机制，利用大模型根据当前标题进行头脑风暴，生成多个伪指令。通过这些伪指令检索附件段落，从而生成更具细节性的文本段落。
   ```mermaid
    graph LR
        A[需求说明] --> |1|B[生成大纲]
        B --> C[摘取标题]
        C --> |2|D[标题指令生成]
        D --> |3|E[关联检索附件]
        E --> |4|F[生成段落]
   ```

+ **获取数据**
  上述流程1、2、3、4涉及大模型推理，旨在生成细粒度的大纲。具体而言，该流程根据用户需求和当前标题，生成更为具体且与标题紧密相关的指令。随后，依据这些指令，从附件中精准地提取相关信息。最终，结合标题与所提取的关联信息，生成详尽的文本段落。
  在真实场景中，当我们面对大量与某个领域相关的附件时，可以采用以下`Self-instruct`来高效获取样本：
  `Self-instruct`是一种用于生成指令数据的方法,通过模型自身生成指令或相应的输入-输出对，从而构建一个大规模的、多样化的数据集。为了使生成的指令更具针对性，可以引入`标题`等元数据信息，以进一步增强指令的精确性和实用性。
  ```mermaid
  graph LR
      A[解析附件] --> B[粒度分块]
      B --> C[数据清洗]
      C --> D[数据过滤]
      D --> E[数据筛选]
      E --> F[标题提取]
      F --> G[问题生成]
      G --> H[生成段落]
  ```
  + **文件拆分**
  首先，使用`llama_index`对文件进行解析，`llama-hub`提供了多种文件格式的解析支持。解析后的文件将以句子为粒度进行分块。接着，对每个分块数据分别执行清洗和筛选操作（基于`neo`开源项目的代码），以确保数据的高质量，从而为后续的模型训练提供优质的输入数据。

  + **数据清洗** 
  该函数用于对输入文本进行格式统一和清洗操作，确保文本符合特定的格式要求。该函数用于从文本中移除 URL 链接，确保文本中不包含任何形式的 URL。
    1. *中文括号检测与比例计算*
    2. *全角符号转半角符号*
    3. *移除常见 URL*

  + **数据筛选**
  主要实现了三个功能模块，分别用于根据自定义规则、`CCNet`规则和重复内容规则对文本进行过滤和验证。以下是每个模块的主要功能总结：
    1. *自定义规则过滤*： 句子数量、平均词长、 数字词比例  ......
    2. *CCNet规则过滤*： 语言识别模型评分
    3. *重复内容规则过滤*：n-gram重复字符比例

  + **提示模板**
    
    <details>
      <summary>
        <b>标题提取提示词</b>
      </summary>
    
    ```python
    """
    Context: {context_str}. 
    根据上下文中所有独特的实体、标题或主题，生成一个简洁的标题。
    Title: """
    ```

    <details>
      <summary>
        <b>指令生成提示词</b>
      </summary>
    
      ```python
      """
      {context_str}
      请针对这篇文章，提出{num_questions}个中文问题，保证问题多样性、尽可能覆盖全部内容，格式如下: "1: ", "2: ", ...
      """
      ```

    <details>
      <summary>
        <b>段落提示词</b>
      </summary>
    
      ```python
        """
        # 用户指令
           {instruction}
        # 给定标题：
           {title}
        # 任务:
           根据用户指令，生成一个详尽的段落，该段落需要详细阐述所提供的标题所涵盖的内容。
        # 参考信息
           [文档片段]({content}) 
        # 写作指南:
              步骤1: 构建一个连贯的段落，直接针对标题展开。
              步骤2: 在主体部分，需深度挖掘子标题的内涵，通过具体事例、数据或理论分析，增强论述的说服力和可读性。
              步骤3: 审阅段落，确保清晰、连贯，并紧扣子标题的重点。
        """     
      ```
    
  + **字段名称**
    
    在当前目录下，提供一个用于生成合成数据的脚本。运行 `pipeline.py` 脚本时，传入文件数据目录，即可获取样本集合。
    
      <details>
        <summary>
          <b>字段名称</b>
        </summary>
    
    ~~~python
    ```python
          {
              "instruction": "<具体指令>",
              "title": "<当前标题信息>",
              "content": "<检索相关信息或文件块信息>",
              "output": "<输出段落信息>"
          },
    ```
    ~~~
    
      微调数据的格式通常为`alpaca`或`sharegpt`，述字段格式进行转化。
      </details>

------

#### **指令微调**
在此基础上，我们选择`longwriter`作为基础模型，因为它具备出色的长文本输出能力。其背后的技术原理，您可以参考我撰写的详细解析：[一网打尽智普长文本训练技术](https://mp.weixin.qq.com/s/7vr-JalvF94fxevOIcZDYQ)。 此外，训练数据中混合了`longwriter-6k-filtered`数据集，主要原因是该部分数据质量较高，能够有效提升模型的表现和生成结果的准确性。

  <details>
    <summary>
        <b>微调样本样例</b>
    </summary>

```python
  [
    {
      "messages": [
            {
                "role": "user",
                "content": "我当前33岁，面部已出现大量雀斑，我推测这可能是遗传因素所致。我对此表示忧虑，雀斑的形成确有遗传倾向吗？有何治疗方法？"
            },
            {
                "role": "assistant",
                "content": "通常，雀斑的产生与体内内分泌失调及新陈代谢状态不佳有关，而非单纯的遗传因素。您与您的母亲在生活环境中以及日常习惯上的相似之处，可能共同促成了雀斑的生成。建议您注重规律作息，避免过度疲劳，减轻心理压力，并定期敷用面膜，以助于减轻雀斑的显现。"
            }
  ]
```
</details>

提供三种灵活的微调方案，以满足不同场景和需求：

1. **Sequence Packing 方法**：对于需要高效处理长序列的场景，我们推荐使用 `LongWriter` 官方代码仓库。该方案通过序列打包技术，能够显著提升长文本处理的效率，特别适合需要长上下文依赖的任务。
2. **Llama Factory 仓库**：一个功能强大且易于上手的微调工具，该仓库提供了丰富的预训练模型和微调接口，支持多种任务类型，且具备良好的扩展性，适合快速迭代。
3. **本仓库**：如果您需要对数据处理流程或模型训练进行深度定制，本仓库提供了灵活的代码框架，允许您根据具体需求修改数据处理方法(collate_fn)和训练方法(trainer)。

+ ##### Full-parameter finetuning

  全参数参数微调需要在整个训练过程中更新LLM的所有参数。请在 shell 脚本中指定正确的 MODEL 路径、DATA 路径等。

  ```shell
  sh finetune.sh
  ```

+ ##### LoRA finetuning

  LoRA 允许仅通过更新一小部分参数进行轻量级模型调整。本仓库基于 `peft` 提供 LoRA 实现。要启动您的训练，请运行以下脚本：

  ```shell
  sh finetune_lora.sh
  ```